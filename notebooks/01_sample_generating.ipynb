{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe051b04-276e-4ecc-9e65-d87f7c2b99ad",
   "metadata": {},
   "source": [
    "***************************************************************************************\n",
    "Jupyter Notebooks from the Metadata for Everyone project\n",
    "\n",
    "Code:\n",
    "* Dennis Donathan II (https://orcid.org/0000-0001-8042-0539)\n",
    "\n",
    "Project team: \n",
    "* Juan Pablo Alperin (https://orcid.org/0000-0002-9344-7439)\n",
    "* Dennis Donathan II (https://orcid.org/0000-0001-8042-0539)\n",
    "* Mike Nason (https://orcid.org/0000-0001-5527-8489)\n",
    "* Julie Shi (https://orcid.org/0000-0003-1242-1112)\n",
    "* Marco Tullney (https://orcid.org/0000-0002-5111-2788)\n",
    "\n",
    "Last updated: xxx\n",
    "***************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a745ac0a",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "Here we will utiilize the Crossref REST API to generate our random sample. We'll use the habanero library, a wrapper for the Crossref API, to make the process easier. More info on the package can be found here: https://github.com/sckott/habanero\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e39cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from habanero import Crossref, WorksContainer\n",
    "import pandas as pd\n",
    "import time\n",
    "from pathlib import Path\n",
    "# Directories for storing the data\n",
    "data_dir = Path('../data')\n",
    "input_dir = data_dir / 'input'\n",
    "# setting up the our queries. In order to be added to the 'Polite' pool for the API, add an email address so that they\n",
    "# can contact you should any problems arise.\n",
    "# cr = Crossref(mailto='youremail@here.com)\n",
    "# We are only interesting in journal articles and as such have a filter toward that end. \n",
    "# Additionally, we are utilizing the 'sample' feature that allows us to grab random works. Limit 100 per request\n",
    "search = cr.works(filter = {'type':'journal-article'}, sample=100)\n",
    "# The WorksContainer class allows us to easily parse through the responses so that way we can extract the records themselves\n",
    "# and not all of the metadata associated with the API call.\n",
    "x = WorksContainer(search)\n",
    "# We'll set up a dataframe with this initial search just to make sure the format looks good and verifying the query results\n",
    "df = pd.DataFrame(data= x.works)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824f242-8adb-4d72-81c3-6e94e29fa7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note: If you take a new sample, the results will also change slightly. \n",
    "# To repeat our calculations, use our data sample. \n",
    "# To check our results with a new analysis, get a new sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dfd2b3",
   "metadata": {},
   "source": [
    "## Getting the full Sample\n",
    "The initial query looks good, so we'll move on to getting the full sample. We're looking for 500,000 unique records. This may take some time to collect, so best to run it overnight or in the background.\n",
    "\n",
    "We want 500,000 unique records, so we'll set up our loop to count the number of unique DOIs we have and stop once we have 500,000. We'll have duplicates, and we'll handle those in the data cleaning notebook (among other things).\n",
    "\n",
    "Since this can take a while, we'll want to build in a safety net against errors and timeouts. If an error occurs, the data is saved, the script is given some sleep time, then it begins again.\n",
    "\n",
    "Once it has hit 500,000 unique records, we'll save the file and move on to cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eef82b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(set(df['DOI'])) < 530000:\n",
    "    try:\n",
    "        search = cr.works(filter={'type':'journal-article'}, sample=100)\n",
    "        x = WorksContainer(search)\n",
    "        for work in x.works:\n",
    "            df.loc[len(df)] = work\n",
    "    except:\n",
    "        df.to_csv(input_dir / '01_raw_data.csv')\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(input_dir / '01_raw_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
