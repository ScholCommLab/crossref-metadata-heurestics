{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import time\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "import modin.pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm  \n",
    "from pathlib import Path\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "data_dir = Path('~/Docs/Metadata For Everyone/data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 17:22:59,700\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "indexed                   {'date-parts': [[2022, 4, 5]], 'date-time': '2...\n",
       "reference-count                                                           3\n",
       "publisher                                                       Elsevier BV\n",
       "license                   [{'start': {'date-parts': [[2014, 12, 1]], 'da...\n",
       "content-domain            {'domain': ['clinicalkey.fr', 'em-consulte.com...\n",
       "short-container-title                      ['La Revue de Médecine Interne']\n",
       "abstract                                                                NaN\n",
       "DOI                                            10.1016/j.revmed.2014.10.351\n",
       "type                                                        journal-article\n",
       "created                   {'date-parts': [[2014, 12, 4]], 'date-time': '...\n",
       "page                                                                   A198\n",
       "update-policy                  http://dx.doi.org/10.1016/elsevier_cm_policy\n",
       "source                                                             Crossref\n",
       "is-referenced-by-count                                                    0\n",
       "title                     ['Pemphigus paranéoplasique chez un homme jeun...\n",
       "prefix                                                              10.1016\n",
       "volume                                                                   35\n",
       "author                    [{'given': 'N.', 'family': 'Peiffer-Smadja', '...\n",
       "member                                                                 78.0\n",
       "published-online                                                        NaN\n",
       "container-title                            ['La Revue de Médecine Interne']\n",
       "language                                                                 fr\n",
       "link                      [{'URL': 'https://api.elsevier.com/content/art...\n",
       "deposited                 {'date-parts': [[2018, 9, 27]], 'date-time': '...\n",
       "score                                                                   0.0\n",
       "resource                  {'primary': {'URL': 'https://linkinghub.elsevi...\n",
       "issued                                         {'date-parts': [[2014, 12]]}\n",
       "references-count                                                          3\n",
       "URL                          http://dx.doi.org/10.1016/j.revmed.2014.10.351\n",
       "relation                                                                NaN\n",
       "ISSN                                                          ['0248-8663']\n",
       "issn-type                         [{'value': '0248-8663', 'type': 'print'}]\n",
       "published                                      {'date-parts': [[2014, 12]]}\n",
       "assertion                 [{'value': 'Elsevier', 'name': 'publisher', 'l...\n",
       "funder                                                                  NaN\n",
       "published-print                                {'date-parts': [[2014, 12]]}\n",
       "reference                 [{'key': '10.1016/j.revmed.2014.10.351_bib0005...\n",
       "alternative-id                                        ['S0248866314010066']\n",
       "subject                           ['Gastroenterology', 'Internal Medicine']\n",
       "article-number                                                          NaN\n",
       "issue                                                                   NaN\n",
       "journal-issue                                                           NaN\n",
       "subtitle                                                                NaN\n",
       "original-title                                                          NaN\n",
       "archive                                                                 NaN\n",
       "editor                                                                  NaN\n",
       "published-other                                                         NaN\n",
       "Name: 255126, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_dir / '01_raw_data.csv')\n",
    "df.sample().iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(raylet)\u001b[0m Spilled 3212 MiB, 28 objects, write throughput 287 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 4695 MiB, 32 objects, write throughput 234 MiB/s.\n"
     ]
    }
   ],
   "source": [
    "already_read = pd.read_csv(data_dir / 'allv3.csv', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: `DataFrame._repr_html_` for empty DataFrame is not currently supported by PandasOnRay, defaulting to pandas implementation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indexed</th>\n",
       "      <th>reference-count</th>\n",
       "      <th>publisher</th>\n",
       "      <th>license</th>\n",
       "      <th>content-domain</th>\n",
       "      <th>short-container-title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>DOI</th>\n",
       "      <th>type</th>\n",
       "      <th>created</th>\n",
       "      <th>...</th>\n",
       "      <th>alternative-id</th>\n",
       "      <th>subject</th>\n",
       "      <th>article-number</th>\n",
       "      <th>issue</th>\n",
       "      <th>journal-issue</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>original-title</th>\n",
       "      <th>archive</th>\n",
       "      <th>editor</th>\n",
       "      <th>published-other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [indexed, reference-count, publisher, license, content-domain, short-container-title, abstract, DOI, type, created, page, update-policy, source, is-referenced-by-count, title, prefix, volume, author, member, published-online, container-title, language, link, deposited, score, resource, issued, references-count, URL, relation, ISSN, issn-type, published, assertion, funder, published-print, reference, alternative-id, subject, article-number, issue, journal-issue, subtitle, original-title, archive, editor, published-other]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 47 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.DOI.str.lower() == '10.4138/atlgeol.2019.011']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_list = list(set(df.DOI.str.lower()).difference(already_read[1].str.lower()))\n",
    "len(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530044"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(already_read[1].str.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|█████     | 266030/530282 [24:02<12:33, 350.70it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(total=20000)\n",
    "\n",
    "new_dois = set()\n",
    "i = 0\n",
    "try:\n",
    "    with open(data_dir / 'doi_file_2.csv', 'r') as file:\n",
    "        for line in file:\n",
    "            i+=1\n",
    "            progress_bar.update(1)\n",
    "            if i < 265141:\n",
    "                continue\n",
    "            \n",
    "            # Split each line at the first comma and take the part before it\n",
    "            doi = line.split(',', 1)[0].strip().lower()\n",
    "            if doi[:3] != '10.':\n",
    "                print(doi)\n",
    "                break\n",
    "\n",
    "            new_dois.add(doi)\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(line)\n",
    "    raise\n",
    "            \n",
    "progress_bar.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_first_column(file_path):\n",
    "    column_data = []\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for row in csvreader:\n",
    "            if row:  # Checking if the row is not empty\n",
    "                column_data.append(row[0])  # Appending the first element of each row to the list\n",
    "    return column_data\n",
    "\n",
    "def fetch_data(doi: str):\n",
    "    \"\"\"Request function to query Crossref API.\n",
    "\n",
    "    Args:\n",
    "        doi (str): The DOI of an item, used for querying Crossref API\n",
    "\n",
    "    Returns:\n",
    "        JSON: with r.status_code == 200, returns JSON response\n",
    "        None: r.status_code == 404 will return None as the resource was not found\n",
    "        function: r.status_code == 504 returns the function to retry the query\n",
    "    \"\"\"\n",
    "    base_url = 'https://doi.crossref.org/search/doi'\n",
    "    params = {'pid': 'juan@alperin.ca',\n",
    "             'format': 'unixsd',\n",
    "             'doi': doi}\n",
    "    try:\n",
    "        r = requests.get(base_url, params=params)\n",
    "        if r.status_code == 200:\n",
    "            soup = BeautifulSoup(r.content.decode('utf-8').replace('\\n', '').replace('\\r', ''), 'xml')\n",
    "            return  ''.join(str(tag) for tag in soup.find_all()).replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "        elif r.status_code == 404:\n",
    "            return None  \n",
    "        elif r.status_code == 504:\n",
    "            print(r.status_code)\n",
    "            time.sleep(1)\n",
    "            return fetch_data(doi)\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching DOI {doi}: {e}\")\n",
    "        return None\n",
    "    \n",
    "def get_crossref(id_list: list):\n",
    "    \"\"\"Primary function for querying Crossref API and collecting responses\n",
    "\n",
    "    Args:\n",
    "        id_list (list): List of all DOIs to be queried.\n",
    "    \"\"\"\n",
    "    chunk_size = 5000\n",
    "    tmp = []\n",
    "    \n",
    "    print(f\"Going after: {len(id_list)}.\")\n",
    "    \n",
    "    file_path = data_dir / 'doi_file.csv'\n",
    "    if file_path.is_file():\n",
    "        print(f\"The file {file_path} exists.\")\n",
    "        # cut -d',' -f1 doi_file.csv > dois_read.csv\n",
    "    else:\n",
    "        pd.DataFrame(columns=['DOI', 'message']).to_csv(file_path, mode='w', index=False)\n",
    "\n",
    "#     already_read = pd.read_csv(data_dir / 'dois_read.csv')\n",
    "#     print(f\"Already read: {len(already_read)}.\")\n",
    "#     id_list = list(set(id_list).difference(already_read.DOI.str.lower()))\n",
    "    print(f\"Going after: {len(id_list)}.\")\n",
    "        \n",
    "    # Record the starting time\n",
    "    start_time = time.time()\n",
    "    \n",
    "   \n",
    "    with tqdm(total=len(id_list)) as pbar:\n",
    "        for i, doi in enumerate(id_list):\n",
    "            try:\n",
    "                result = fetch_data(doi)\n",
    "                if result is not None:\n",
    "                    tmp.append({'DOI': doi, 'message': result})\n",
    "                    \n",
    "                if i % chunk_size == 0 or (i+1) == len(id_list):\n",
    "                    pd.DataFrame(tmp).to_csv(data_dir / 'doi_file.csv', mode='a', index=False, header=False)\n",
    "                    tmp = []\n",
    "                    end_time = time.time()\n",
    "                    if i/3 > (end_time - start_time):\n",
    "                        pause = i/3 - (end_time - start_time) \n",
    "                        print(f\"Sleeping: {int(pause)} seconds\")\n",
    "                        time.sleep(pause)\n",
    "\n",
    "                pbar.update(1)\n",
    "            except KeyboardInterrupt:\n",
    "                if len(tmp) > 1: \n",
    "                    pd.DataFrame(tmp).to_csv(data_dir / 'doi_file.csv', mode='a', index=False, header=False)                \n",
    "                raise\n",
    "            except Exception as err:                \n",
    "                print(err)\n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/106402 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going after: 106402.\n",
      "Going after: 106402.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106402/106402 [13:19:05<00:00,  2.22it/s]   \n"
     ]
    }
   ],
   "source": [
    "get_crossref(id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching data ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "x = df.sample().iloc[0]['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = fetch_data('10.4138/atlgeol.2019.011')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_all_xml_tags(xml_string, tag_name):\n",
    "    pattern = re.compile(r'<{}([^>]*)>(.*?)</{}>'.format(tag_name, tag_name), re.DOTALL)\n",
    "    matches = re.findall(pattern, xml_string)\n",
    "    \n",
    "    tag_list = []\n",
    "    for attributes, content in matches:\n",
    "        attributes_dict = dict(re.findall(r'\\b(\\S+?)\\s*=\\s*[\\'\"](.*?)[\\'\"]', attributes))\n",
    "        tag_list.append({\n",
    "            'attributes': attributes_dict,\n",
    "            'content': content\n",
    "        })\n",
    "\n",
    "    return tag_list\n",
    "\n",
    "# Example usage\n",
    "xml_string = x\n",
    "desired_tag_name = 'jats:abstract'\n",
    "\n",
    "result = find_all_xml_tags(xml_string, desired_tag_name)\n",
    "for i, match in enumerate(result):\n",
    "    print(\"Match {}: {}\".format(i + 1, match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110498</th>\n",
       "      <td>10.1186/s13102-023-00654-y</td>\n",
       "      <td>&lt;crossref_result version=\"3.0\" xmlns=\"http://w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0  \\\n",
       "110498  10.1186/s13102-023-00654-y   \n",
       "\n",
       "                                                        1  \n",
       "110498  <crossref_result version=\"3.0\" xmlns=\"http://w...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dois = set()\n",
    "\n",
    "df = pd.read_csv(data_dir / 'outfile_5.csv', header=None)\n",
    "dois = set(df[0])\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing a:   3%|▎         | 20000/620000 [01:54<59:10, 168.97item/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found dois: 354999"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing a:   3%|▎         | 20925/620000 [02:01<1:17:00, 129.65item/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found dois: 355924"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing a:   7%|▋         | 40925/620000 [04:15<1:14:16, 129.94item/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found dois: 375924"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing a:  10%|▉         | 60925/620000 [06:29<57:11, 162.91item/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found dois: 395924"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing a:  13%|█▎        | 80925/620000 [08:15<1:02:07, 144.63item/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found dois: 410000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing a:   0%|          | 0/620000 [00:00<?, ?item/s]                \n"
     ]
    }
   ],
   "source": [
    "def list_files_with_prefix(directory, prefix):\n",
    "    file_list = [f for f in os.listdir(directory) if f.startswith(prefix) and os.path.isfile(os.path.join(directory, f))]\n",
    "    return file_list\n",
    "\n",
    "def reverse_sorted_files_by_mtime(directory, prefix):\n",
    "    file_list = list_files_with_prefix(directory, prefix)\n",
    "    file_list.sort(key=lambda x: os.path.getmtime(os.path.join(directory, x)), reverse=True)\n",
    "    return file_list\n",
    "\n",
    "# Example usage\n",
    "\n",
    "file_prefixes = ['4', '3', '2', '1', 'a']\n",
    "\n",
    "\n",
    "for file_prefix in file_prefixes: \n",
    "    clear_output(wait=True)\n",
    "    sorted_files = reverse_sorted_files_by_mtime(data_dir, \"doi_{}\".format(file_prefix))\n",
    "    # Using tqdm with a multiplier for multiple updates per iteration\n",
    "    with tqdm(total=20000*len(sorted_files), desc=\"Processing {}\".format(file_prefix), unit=\"item\") as pbar:\n",
    "        with open(data_dir / 'outfile_{}.csv'.format(file_prefix), 'a') as outfile:\n",
    "            for filename in sorted_files:\n",
    "                one_new_in_file = False\n",
    "                \n",
    "                # Your processing code goes here\n",
    "                with open(data_dir / filename, 'r') as infile:\n",
    "                    for line in infile:\n",
    "                        # Update the progress bar\n",
    "                        pbar.update(1)\n",
    "                        \n",
    "                        # Split each line at the first comma and take the part before it\n",
    "                        doi = line.split(',', 1)[0].strip().lower()\n",
    "                        if doi[:3].lower() == 'doi':\n",
    "                            continue\n",
    "                        \n",
    "                        elif doi[:3] != '10.':\n",
    "                            print(\"Something went wrong when reading: {}\".format(doi))\n",
    "                            break\n",
    "                        \n",
    "                        # skip line if we already have the DOI\n",
    "                        if doi in dois: \n",
    "                            continue\n",
    "\n",
    "                        # copy the line to a new outfile and record that we already have the DOI\n",
    "                        outfile.write(line)   \n",
    "                        dois.add(doi)                            \n",
    "                        # if we don't find any new DOIs in the whole file, stop checking\n",
    "                        # the rest of the files in this set\n",
    "                        one_new_in_file = True\n",
    "\n",
    "                # if no new DOIs found in the whole file, move onto next prefix\n",
    "                if one_new_in_file == False:\n",
    "                    break\n",
    "                tqdm.write(f\"Found dois: {len(dois)}\", end=\"\")\n",
    "                      \n",
    "        pbar.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "junk after document element: line 1, column 5943 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Users/juan/.pyenv/versions/myenv3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3418\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-32-2524e21ea6a6>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    root = ET.fromstring(x)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/juan/.pyenv/versions/3.8.6/lib/python3.8/xml/etree/ElementTree.py\"\u001b[0;36m, line \u001b[0;32m1320\u001b[0;36m, in \u001b[0;35mXML\u001b[0;36m\u001b[0m\n\u001b[0;31m    parser.feed(text)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m junk after document element: line 1, column 5943\n"
     ]
    }
   ],
   "source": [
    "root = ET.fromstring(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<person_name contributor_role=\"author\" sequence=\"first\"> <given_name>Mirela</given_name> <surname>Feurdean</surname> </person_name>\n",
      "<given_name>Mirela</given_name>\n",
      "<surname>Feurdean</surname>\n",
      "<person_name contributor_role=\"author\" sequence=\"additional\"> <given_name>Daniel</given_name> <surname>Matassa</surname> </person_name>\n",
      "<given_name>Daniel</given_name>\n",
      "<surname>Matassa</surname>\n",
      "<person_name contributor_role=\"author\" sequence=\"additional\"> <given_name>Mohleen</given_name> <surname>Kang</surname> </person_name>\n",
      "<given_name>Mohleen</given_name>\n",
      "<surname>Kang</surname>\n",
      "<person_name contributor_role=\"author\" sequence=\"additional\"> <given_name>Genevieve</given_name> <surname>Matthews</surname> </person_name>\n",
      "<given_name>Genevieve</given_name>\n",
      "<surname>Matthews</surname>\n",
      "<person_name contributor_role=\"author\" sequence=\"additional\"> <given_name>Neil</given_name> <surname>Kothari</surname> </person_name>\n",
      "<given_name>Neil</given_name>\n",
      "<surname>Kothari</surname>\n"
     ]
    }
   ],
   "source": [
    "for person in soup.crossref.contributors.findChildren():\n",
    "    print(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
