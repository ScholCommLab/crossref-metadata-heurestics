{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa28d027-7ae4-4458-b17b-552b46e3f161",
   "metadata": {},
   "source": [
    "***************************************************************************************\n",
    "Jupyter Notebooks from the Metadata for Everyone project\n",
    "\n",
    "Code:\n",
    "* Dennis Donathan II (https://orcid.org/0000-0001-8042-0539)\n",
    "\n",
    "Project team: \n",
    "* Juan Pablo Alperin (https://orcid.org/0000-0002-9344-7439)\n",
    "* Dennis Donathan II (https://orcid.org/0000-0001-8042-0539)\n",
    "* Mike Nason (https://orcid.org/0000-0001-5527-8489)\n",
    "* Julie Shi (https://orcid.org/0000-0003-1242-1112)\n",
    "* Marco Tullney (https://orcid.org/0000-0002-5111-2788)\n",
    "\n",
    "Last updated: xxx\n",
    "***************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efae076",
   "metadata": {},
   "source": [
    "# Language Detection\n",
    "\n",
    "In the previous notebook, we found that a significant number of records are missing a stated *language* within the record. Additionally, we found in our work in phase 1 of our project (see Shi, J., Nason, M., Tullney, M., & Alperin, J. P. (2023). Identifying Metadata Quality Issues Across Cultures. SocArXiv. https://doi.org/10.31235/osf.io/6fykh)) it is not uncommon for the stated language of the record to be inaccurate. This could be for a variety of reasons: lack of clarity as to what *language* is actually referring to (i.e. the language of the item, container, or the metadata record itself), perhaps some level of increased discoverability if the work is labeled with **en** (English).\n",
    "\n",
    "To examine these issues more closely, we will detect the languages used within the records, compare that against their stated languages, and see any patterns that emerge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a48e1-349d-497a-92c7-091dff8dd170",
   "metadata": {},
   "source": [
    "## Preparation \n",
    "First we will import several of the necessary packages, set up our directory, and import our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c388426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns # data visualizations\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd #Creating dataframe and manipulating data\n",
    "from py3langid.langid import LanguageIdentifier, MODEL_FILE\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a4cbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Directory\n",
    "data_dir = Path('../data')\n",
    "input_dir = data_dir / 'input'\n",
    "output_dir = data_dir / 'output'\n",
    "# Loading in dataset\n",
    "df = pd.read_csv(output_dir / '03_labeled_data.csv', parse_dates=['created', 'deposited'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814a2f3a",
   "metadata": {},
   "source": [
    "## Publisher Bins\n",
    "Not all publishers are of the same size, obviously. Thus it makes it difficult to draw any meaningful comparisons between Elsevier BV and, for instance, the South African National Biodiversity Institute. So while we cannot have multiple large publishers such as Elsevier BV, we can group publishers to create bins of similar size and make comparisons between the publisher sizes.\n",
    "\n",
    "We will create five bins, or groups, with each bin having ~20000 records. The `XS` bin has publishers who have 0 to 75 records in the dataset, `S`: 76-1000, `M`:1001-5000, `L`:5001-10000 (this bin is comprised of Springer and Wiley exclusively), `XL`:10000+ (Elsevier BV exclusively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc3b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_count = df.publisher.value_counts()\n",
    "cut_bins = pd.cut(pub_count, bins=[0,75,1000,5000,10050,20000], labels=['XS','S','M','L','XL'])\n",
    "def bin_applicator(record):\n",
    "    try:\n",
    "        pub = record['publisher']\n",
    "        locator = cut_bins.loc[cut_bins.index == pub]\n",
    "        locList = locator.to_list()\n",
    "        return locList[0]\n",
    "    except:\n",
    "        return None\n",
    "df['publisher_bin'] = df.apply(bin_applicator, axis=1)\n",
    "df.publisher_bin.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce3209",
   "metadata": {},
   "source": [
    "## Detecting Languages\n",
    "We'll use `py3langid` as we did previously, with the same language list as before. While `py3langid` is optomized for python3 and is several times faster than the original, it is important to note that due to the nature of the language detection we will be doing, it may take a minute or two, but no longer than that. We will check each record across three fields, *abstract*, *title*, and *container-title*. These are the fields that have some of the most text, and thus can give us the most confident results. We'll set a probability threshold of `.95` to help insure that we are only saying a language is present when the model is very confident.\n",
    "\n",
    "### Matching\n",
    "After detecting the langauge used in the records, we will then see if the `detected_lang` matches the record's stated *language*. In doing this we will label the record with a `0` if the stated language matches detected language, `1` if the stated language **does not** match detected language), or `2` if the multiple detected languages, but one of the detected languages matches the stated language). \n",
    "\n",
    "### Language Type\n",
    "Finally, we will apply an additional code to each record: `0` if the detected language is English, `1` if the detected language is any single non-english language, `2` for multilingual records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867bce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = LanguageIdentifier.from_pickled_model(MODEL_FILE, norm_probs = True)\n",
    "lang_list = ['af', 'am', 'ar', 'as', 'az', 'be', 'bg', 'bn', 'br', \n",
    "             'bs', 'ca', 'cs', 'cy', 'da', 'de', 'dz', 'el', 'en', 'eo', \n",
    "             'es', 'et', 'eu', 'fa', 'fi', 'fo', 'fr', 'ga', 'gl', 'gu', \n",
    "             'he', 'hi', 'hr', 'ht', 'hu', 'hy', 'is', 'it', 'ja', 'jv', \n",
    "             'ka', 'kk', 'km', 'kn', 'ko', 'ku', 'ky', 'la', 'lb', 'lo', \n",
    "             'lt', 'lv', 'mg', 'mk', 'ml', 'mn', 'mr', 'ms', 'mt', 'ne', \n",
    "             'nl', 'no', 'oc', 'or', 'pa', 'pl', 'ps', 'pt', 'qu', 'ro', \n",
    "             'ru', 'rw', 'se', 'si', 'sk', 'sl', 'sq', 'sr', 'sv', 'sw', \n",
    "             'ta', 'te', 'th', 'tl', 'tr', 'ug', 'uk', 'ur', 'vi', 'vo', \n",
    "             'wa', 'xh', 'zh', 'zu']\n",
    "identifier.set_languages(langs=lang_list)\n",
    "# Check across multiple fields within each record for the languages present.\n",
    "def record_lang_checker(record):\n",
    "    #These fields have the most text which will provide the most accurate language detection\n",
    "    fields = ['abstract', 'title', 'container-title']\n",
    "    lang_list = []\n",
    "    for col in fields:\n",
    "        try:\n",
    "            detect = identifier.classify(record[col].lower())\n",
    "            # Setting a .95 probability threshold for asserting the language is indeed in the record\n",
    "            if detect[1] > .95:\n",
    "                lang_list.append(detect[0])\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "    #If no language is detected, return None\n",
    "    if len(lang_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        #Returning all of the detected languages for each record\n",
    "        return list(set(lang_list))\n",
    "df['detected_lang'] = df.apply(record_lang_checker, axis=1)\n",
    "detected_languages = df.explode('detected_lang')\n",
    "detected_languages.detected_lang.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5827ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_match(record):\n",
    "    try:\n",
    "        #Filtering out records with no stated language\n",
    "        if record['language'] == np.nan:\n",
    "            return 1\n",
    "        else:\n",
    "            #checking if stated language matches detected language\n",
    "            if record['language'] in record['detected_lang']:\n",
    "                #Stated language is within the detected languages, but there are multiple languages\n",
    "                #present in the record\n",
    "                if len(record['detected_lang']) > 1:\n",
    "                    return 2\n",
    "                #Stated language matches detected language\n",
    "                else:\n",
    "                    return 0\n",
    "            #Stated and detected languages do not match\n",
    "            else:\n",
    "                return 1\n",
    "    except:\n",
    "        return None\n",
    "df['lang_match'] = df.apply(detection_match, axis=1)\n",
    "df.lang_match.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ffbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_type(record):\n",
    "    try:\n",
    "        # Records that we've already determined have multilingual content can be assigned\n",
    "        # their categorization and filtered out\n",
    "        if record['abstract_multi_lang'] == 1:\n",
    "            return 'Multilingual'\n",
    "        else:\n",
    "            #Checking the monolingual records if they are english or non-english\n",
    "            if len(record['detected_lang']) == 1:\n",
    "                if record['detected_lang'][0] == 'en':\n",
    "                    return 'Monolingual English'\n",
    "                else:\n",
    "                    return 'Monolingual Non-English'\n",
    "            else:\n",
    "                #Multilingual records\n",
    "                if len(record['detected_lang']) > 1:\n",
    "                    return 'Multilingual'\n",
    "                else:\n",
    "                    return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['lang_type'] = df.apply(lang_type, axis=1)\n",
    "df.lang_type.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba21fe",
   "metadata": {},
   "source": [
    "## Differences in Errors Between Language Types\n",
    "After detecting the languages and coding the records, we can see that there are a large number of records in which the `detected_lang` does not match the stated language. One possible explanation is the high number of records that simply do not have a stated language. We will explore this below.\n",
    "\n",
    "Additionally, we can see that English is the predominant language of the dataset. Next, we'll take a look at errors per record in regards to the different language types: English-monolingual, Non-English-monolingual, and Multilingual.\n",
    "\n",
    "First, we'll take a look at the number of errors per language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e31184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_langs = detected_languages.groupby('detected_lang')\n",
    "group_total_errors = grouped_langs.agg({'total_errors': 'sum', 'DOI': 'count'}).sort_values(by='DOI', ascending=False)\n",
    "group_total_errors[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1532e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('paper')\n",
    "top_20 = group_total_errors.sort_values(by='total_errors', ascending=False)[:20]\n",
    "t20plt = sns.barplot(data=top_20, x=top_20.index, y='total_errors')\n",
    "t20plt.set_xticklabels(t20plt.get_xticklabels(), rotation=40, ha='right', fontsize=10)\n",
    "t20plt.set_title('Total Errors by Language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75487b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we'll remove English, to better visualize other languages\n",
    "no_en = group_total_errors.drop('en')\n",
    "top_20 = no_en.sort_values(by='total_errors', ascending=False)[:20]\n",
    "t20plt = sns.barplot(data=top_20, x=top_20.index, y='total_errors')\n",
    "t20plt.set_xticklabels(t20plt.get_xticklabels(), rotation=40, ha='right', fontsize=10)\n",
    "t20plt.set_title('Total Errors by Language (excl. English)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e5b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we'll change from total errors, to the mean number of errors by language\n",
    "group_avg_errors = grouped_langs.agg({'total_errors': 'mean', 'DOI': 'count'}).sort_values(by='total_errors', ascending=False)\n",
    "group_avg_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531973b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll remove the languages with only a couple of records\n",
    "filtered = group_avg_errors.loc[group_avg_errors.DOI > 5].sort_values(by='total_errors', ascending=False)\n",
    "top_20 = filtered[:20]\n",
    "t20plt = sns.barplot(data=top_20, x=top_20.index, y='total_errors')\n",
    "t20plt.set_xticklabels(t20plt.get_xticklabels(), rotation=40, ha='right', fontsize=10)\n",
    "t20plt.set_title('Errors per Record by Language')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44174aa5",
   "metadata": {},
   "source": [
    "### Individual Languages\n",
    "We see that, as mentioned, English is by far the most represented language in the dataset, and, consequently, has the most errors. Once removing english, we see that German (de), French (fr), Spanish (es), Portugese (pt), and Malay (ms) are the next top 5 in total errors, but that tends to be a reflection of the quantity of  in the dataset.\n",
    "\n",
    "However, when we look at the average (arithmetic mean) of the errors per language, we do see that there are a number of languages from the top 20 of total errors there:\n",
    "\n",
    "Chinese (zh), Russian (ru), Ukranian (uk), Bulgarian (bg), Japanese (ja), Arabic (ar).\n",
    "\n",
    "Now, we'll take a look at the differences between language types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed8a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi = df.loc[df.lang_type == 'Multilingual']\n",
    "non_english = df.loc[df.lang_type == 'Monolingual Non-English']\n",
    "english = df.loc[df.lang_type == 'Monolingual English']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6292767",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_error_rate = multi.total_errors.sum()/len(multi)\n",
    "eng_error_rate = english.total_errors.sum()/len(english)\n",
    "non_eng_error_rate = non_english.total_errors.sum()/len(non_english)\n",
    "\n",
    "print(\"{:0.2f} errors per english, monolingual record\".format(eng_error_rate))\n",
    "print(\"{:0.2f} errors per non-english, monolingual record\".format(non_eng_error_rate))\n",
    "print(\"{:0.2f} errors per multilingual record\".format(multi_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9767700b",
   "metadata": {},
   "source": [
    "To visualize this data, we'll look at the error rates for each of these language types and break them up by their publisher bin.\n",
    "\n",
    "Then We'll take a look at any differences between the languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multilingual Records\n",
    "mlt = sns.catplot(data=multi,x='publisher_bin', y='total_errors', kind='bar', height=4, aspect=.8, order=['XS', 'S', 'M', 'L', 'XL'], errorbar=None)\n",
    "mlt.set_axis_labels('', 'Error per Record')\n",
    "mlt.fig.subplots_adjust(top=0.9)\n",
    "mlt.fig.suptitle('Errors per Record by Publisher Size, Multilingual Records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1edeecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#English Monolingual Records\n",
    "en_only = sns.catplot(data=english, x='publisher_bin', y='total_errors', kind='bar', height=4, aspect=.8, order=['XS', 'S', 'M', 'L', 'XL'], errorbar=None)\n",
    "en_only.set_axis_labels('', 'Error per Record')\n",
    "en_only.fig.subplots_adjust(top=0.9)\n",
    "en_only.fig.suptitle('Errors per Record by Publisher Size, English Only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff191edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-English Monolingual Records\n",
    "non_en_plt = sns.catplot(data=non_english, x='publisher_bin', y='total_errors', kind='bar', height=4, aspect=.8, order=['XS', 'S', 'M', 'L', 'XL'], errorbar=None)\n",
    "non_en_plt.set_axis_labels('', 'Error per Record')\n",
    "non_en_plt.fig.subplots_adjust(top=0.9)\n",
    "non_en_plt.fig.suptitle('Errors per Record by Publisher Size, Monolingual Records (non-English)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e5377",
   "metadata": {},
   "source": [
    "### Differences in Language Types\n",
    "We can see that the publisher bins generally change between groups in a similar fashion. The error rate for all seems to be highest in non-English monolingual records, and at its lowest in English monolingual records.\n",
    "\n",
    "Consistent throughout all groups the XS publisher bin has the highest error rate.\n",
    "\n",
    "Finally, we'll take a look to see any differences in the presence of a stated language between the language types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a0c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_no_lang = (multi.language.isna().sum()/len(multi)) *100\n",
    "eng_no_lang = (english.language.isna().sum()/len(english)) *100\n",
    "non_eng_no_lang = (non_english.language.isna().sum()/len(non_english)) * 100\n",
    "\n",
    "print(\"{:0.2f}% english, monolingual records with no stated language\".format(eng_no_lang))\n",
    "print(\"{:0.2f}% non_english, monolingual records with no stated language\".format(non_eng_no_lang))\n",
    "print(\"{:0.2f}% multilingual records with no stated language\".format(multi_no_lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccdccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_stated(lang):\n",
    "    try:\n",
    "        if lang == 'en':\n",
    "            return 1\n",
    "        elif lang in lang_list:\n",
    "            return 0\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "find_stated = non_english.language.map(lambda x: en_stated(x))\n",
    "enStated = find_stated.sum()\n",
    "nonenStated = len(find_stated.loc[find_stated == 0])\n",
    "non_eng_no_lang = non_english.language.isna().sum()\n",
    "labels = ['English', 'Non-English', 'No Language']\n",
    "data = [enStated, nonenStated, non_eng_no_lang]\n",
    "colors = ['palegreen','skyblue', 'pink']\n",
    "fig = plt.figure(figsize = (10,7))\n",
    "plt.pie(data, labels = labels, autopct='%.1f%%', colors=colors)\n",
    "plt.title('Breakdown of Stated Languages for Non-English, Monolingual Records', fontsize=16)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df75ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_stated = multi.language.map(lambda x: en_stated(x))\n",
    "enStated = find_stated.sum()\n",
    "nonenStated = len(find_stated.loc[find_stated == 0])\n",
    "multi_no_lang = multi.language.isna().sum()\n",
    "labels = ['English', 'Non-English', 'No Language']\n",
    "data = [enStated, nonenStated, multi_no_lang]\n",
    "colors = ['palegreen','skyblue', 'pink']\n",
    "fig = plt.figure(figsize = (10,7))\n",
    "plt.pie(data, labels = labels, autopct='%.1f%%', colors=colors)\n",
    "plt.title('Breakdown of Stated Languages for Multilingual Records', fontsize=16)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21df1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking to see how many multilingual records use english within their records\n",
    "def has_english(langlist):\n",
    "    try:\n",
    "        if 'en' in langlist:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return None\n",
    "eng_multi = multi.detected_lang.map(lambda x: has_english(x))\n",
    "eng_having_rate = (eng_multi.sum()/len(multi)) *100\n",
    "print(\"{:0.2f}% of multilingual records have English as one of their languages\".format(eng_having_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d52a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(output_dir / '04_language_detection.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
