{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9eea90-81ad-4704-9c44-960f6464d217",
   "metadata": {},
   "source": [
    "***************************************************************************************\n",
    "Jupyter Notebooks from the Metadata for Everyone project\n",
    "\n",
    "Code:\n",
    "* Dennis Donathan II (https://orcid.org/0000-0001-8042-0539)\n",
    "\n",
    "Project team: \n",
    "* Juan Pablo Alperin (https://orcid.org/0000-0002-9344-7439)\n",
    "* Dennis Donathan II (https://orcid.org/0000-0001-8042-0539)\n",
    "* Mike Nason (https://orcid.org/0000-0001-5527-8489)\n",
    "* Julie Shi (https://orcid.org/0000-0003-1242-1112)\n",
    "* Marco Tullney (https://orcid.org/0000-0002-5111-2788)\n",
    "\n",
    "Last updated: xxx\n",
    "***************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01402dd8",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "To clean this data set we'll start out by loading the dataset, checking for duplicates, and dropping columns that are not relevant to our analysis.\n",
    "\n",
    "First, we'll load in our packages, set up our directories, and load in the dataset and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfabfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "#Set up directories\n",
    "data_dir = Path('../data')\n",
    "input_dir = data_dir / 'input'\n",
    "output_dir = data_dir / 'output'\n",
    "\n",
    "df = pd.read_csv(input_dir / '01_raw_data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6106906",
   "metadata": {},
   "source": [
    "## Duplicate Records\n",
    "Looking at the shape of the dataset against the number of unique DOIs will let us know just how many duplicate records we have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fd54de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69abaedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(df['DOI']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd49f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping duplicate records\n",
    "df.drop_duplicates(subset=['DOI'], keep='first', inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140334c4",
   "metadata": {},
   "source": [
    "## Editors\n",
    "There are very few records that have a value in the *editor* column. Some of our prior work indicates that this can be a sign of a work that has been mislabeled as a 'journal article'. So we'll explore some of the records with a value in the editor column in order to verify that.\n",
    "\n",
    "We'll set up a dataframe of just those records that have data in the *editor* column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8a6595",
   "metadata": {},
   "source": [
    "Next, we'll search the titles of these records for a few keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee249e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "editorial = df.loc[df.title.str.contains(r'editorial|errata|contents|conference|proceedings|masthead|symposium|abstract|Book Review|preface|title page', \n",
    "                                         regex=True, case=False, na=False)]\n",
    "editorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a311274f",
   "metadata": {},
   "source": [
    "We've found some editorials, Mastheads, conference proceedings, and abstracts. We'll go ahead and drop them from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(editorial.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e265e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6cc2f",
   "metadata": {},
   "source": [
    "## Conferences\n",
    "Looking back at **editorial** we see that there are a couple 'Conferences' and 'Proceedings' in the *container-title* column. Let's take a look at just how many records remain in our dataset are from these journals/containers.\n",
    "\n",
    "Additionally, we see a few records from the journal *ChemInform*, a journal that publishes chemistry abstracts, we'll check to see if any of those records remain as well.\n",
    "\n",
    "We'll use a keyword search in the *container-title* column to find these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbf5ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conferences = df.loc[(df['container-title'].str.contains(r'conference|ChemInform|news|CrossRef Listing of Deleted DOIs', regex=True, case=False)) | (df.publisher == 'EDP Sciences')]\n",
    "conferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d86b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(conferences.index, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff7162a",
   "metadata": {},
   "source": [
    "## Cleaning Dates\n",
    "Here we are going to re-format some of the datetime columns into a more easily parsed format. *Created, deposited* and *published*. Not all records have month and day values for the *published* field, so we'll only take the year from those. For *created* and *deposited* we will have a YYYY-MM-DD format.\n",
    "\n",
    "We've chosen these dates because they reflect certain information that we'll use later on. *Created* is the date when the item was first inserted into the Crossref database. *Deposited* reflects the last time the record was entered by the publisher (potentially with changes to the record but not necessarily the case). *Published* reflects when the item itself was actually published.\n",
    "\n",
    "We'll use a regular expression to extract the dates from each of the records in each of those three columns, then we'll convert them to datetime dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e5c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = ['created', 'deposited']\n",
    "\n",
    "for col in date_columns:\n",
    "    df[col] = df[col].str.extract(r\"\\'(\\d{4}\\S\\d{2}\\S\\d{2})\")\n",
    "    df[col] = pd.to_datetime(df[col], format=\"%Y-%m-%d\")\n",
    "df['published'] = df['published'].str.extract(r\"(\\d{4})\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbbd915",
   "metadata": {},
   "source": [
    "# String slicing\n",
    "Now that the dates are converted, one of the last problems to address are some of the excess character in the *title, short-container-title,* and *container-title* fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca434f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['title', 'short-container-title', 'container-title']\n",
    "for col in cols:\n",
    "    df[col] = df[col].str.slice(start=2, stop=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea8a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68b2b5",
   "metadata": {},
   "source": [
    "## Cleaning XML tags\n",
    "We'll be looking at the abstract column, so it will benefit us to clean out the tags and only have te relevant text for each record. We'll write a quick function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b531a43-fbea-48f5-a64e-5e94f972d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract'][16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d02f413-50e3-4d4f-8777-9e13bfe5b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[16, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb9cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import beautiful soup\n",
    "from bs4 import BeautifulSoup as bs\n",
    "def clean_abstracts(abstract):\n",
    "    try:\n",
    "        soup = bs(abstract, features='lxml')\n",
    "        stripped_strings = soup.get_text()\n",
    "        return stripped_strings\n",
    "    except:\n",
    "        return None\n",
    "stripped_abstracts = df.abstract.map(lambda x: clean_abstracts(x))\n",
    "df['abstract'] = stripped_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract'][16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a349c-3621-4c17-9763-54188422392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all missing values are correctly represented\n",
    "import numpy as np\n",
    "df = df.fillna(value=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb49e1",
   "metadata": {},
   "source": [
    "Looks great! Now we'll save our cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827651bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(input_dir / '02_cleaned_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
