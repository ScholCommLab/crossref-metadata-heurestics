{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09cc80c3-8b6d-4d0a-b9eb-8f4a7ae7c5a7",
   "metadata": {},
   "source": [
    "***************************************************************************************\n",
    "Jupyter Notebooks from the Metadata for Everyone project\n",
    "\n",
    "Code:\n",
    "* Dennis Donathan II (https://orcid.org/0000-0001-8042-0539)\n",
    "\n",
    "Project team: \n",
    "* Juan Pablo Alperin (https://orcid.org/0000-0002-9344-7439)\n",
    "* Dennis Donathan II (https://orcid.org/0000-0001-8042-0539)\n",
    "* Mike Nason (https://orcid.org/0000-0001-5527-8489)\n",
    "* Julie Shi (https://orcid.org/0000-0003-1242-1112)\n",
    "* Marco Tullney (https://orcid.org/0000-0002-5111-2788)\n",
    "\n",
    "Last updated: xxx\n",
    "***************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b4b47f",
   "metadata": {},
   "source": [
    "# Labeling Problems in the Data\n",
    "\n",
    "We identified many issues in Phase 1 of our project (see Shi, J., Nason, M., Tullney, M., & Alperin, J. P. (2023). Identifying Metadata Quality Issues Across Cultures. SocArXiv. https://doi.org/10.31235/osf.io/6fykh). Now we will go through and programatically label the records in this sample if they contain some of those issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e00c59-76ad-4e3c-acc7-7b8a7d2f7943",
   "metadata": {},
   "source": [
    "Start by importing the packages we'll need, setting up our directories, and loading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01df98ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #Creating dataframe and manipulating data\n",
    "from bs4 import BeautifulSoup as bs # for cleaning xml tags\n",
    "import re #regular expressions used for detection of initials\n",
    "from py3langid.langid import LanguageIdentifier, MODEL_FILE #For language detection\n",
    "from nltk.tokenize import sent_tokenize #Tokenizing abstracts during language detection\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e4f1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data Directory\n",
    "data_dir = Path('../data')\n",
    "input_dir = data_dir / 'input'\n",
    "output_dir = data_dir / 'output'\n",
    "# Loading in dataset\n",
    "df = pd.read_csv(input_dir / '02_cleaned_data.csv', \n",
    "                 usecols=['publisher', 'container-title', 'language', 'DOI', 'published', \n",
    "                          'created', 'deposited', 'title', 'author', 'abstract', 'original-title'],\n",
    "                 parse_dates=['created', 'deposited'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a762d8",
   "metadata": {},
   "source": [
    "## Missing Values in Common Fields\n",
    "This is a relatively easy problem to label, so we'll tackle these first.\n",
    "\n",
    "We'll set up a column *'no_author'* and assign `0` to all of the records. Then we will locate the records missing an author and change their value to `1`.\n",
    "\n",
    "Then we'll do the same for the *language, abstract,* and *title* fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1280f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authors\n",
    "df['no_author'] = float(0)\n",
    "df.loc[df.author.isna(), 'no_author'] = float(1)\n",
    "#Language\n",
    "df['no_language'] = float(0)\n",
    "df.loc[df.language.isna(), 'no_language'] = float(1)\n",
    "#Abstracts\n",
    "df['no_abstract'] = float(0)\n",
    "df.loc[df.abstract.isna(), 'no_abstract'] = float(1)\n",
    "#Titles\n",
    "df['no_title'] = float(0)\n",
    "df.loc[df.title.isna(), 'no_title'] = float(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21ac6c-d4b3-4874-9933-c7bfb8cc7512",
   "metadata": {},
   "source": [
    "## Prevalence of missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab9c71b-220f-4150-a154-422368019663",
   "metadata": {},
   "source": [
    "### Missing Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef1cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence_AuMis = (len(df.loc[df.no_author == 1])/len(df)) * 100\n",
    "#prevalence_AuMis # percentage of records with this specific issue\n",
    "print(\"{:0.2f} percent of the article records do not contain an author\".format(prevalence_AuMis))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c51531-5f27-4f18-8607-46696b3c9797",
   "metadata": {},
   "source": [
    "### Missing Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac205d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence_LangMis = (len(df.loc[df.no_language == 1])/len(df)) * 100\n",
    "#prevalence_LangMis # percentage of records with this specific issue\n",
    "print(\"{:0.2f} percent of the article records do not have a language specified\".format(prevalence_LangMis))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdba13a2-b553-4b74-b42a-21cb65c826c0",
   "metadata": {},
   "source": [
    "### Missing Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a7a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence_AbsMis = (len(df.loc[df.no_abstract == 1])/len(df)) * 100\n",
    "#prevalence_AbsMis\n",
    "print(\"{:0.2f} percent of the article records do not contain an abstract\".format(prevalence_AbsMis))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cba29df-245e-4b65-95a9-5b64a1642060",
   "metadata": {},
   "source": [
    "### Missing Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence_TitleMis = (len(df.loc[df.no_title == 1])/len(df)) * 100\n",
    "#prevalence_TitleMis\n",
    "print(\"{:0.2f} percent of the article records do not contain a title\".format(prevalence_TitleMis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246fe905-858a-4520-8d8a-5c5e17f36ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1% is awfully lot, even if title has not always been mandatory. Let's look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7473f2f9-7769-4648-822b-a7ac2dde6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_titles_df = df.loc[df.no_title == 1]\n",
    "len(missing_titles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07beb8c-56bb-4e00-912b-39dde3df741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_titles_df['publisher'].value_counts().tail(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f06eb-957d-41a9-9801-690701beed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_titles_df['published'].value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5152cdbf-5910-458f-b9e3-e39242cfb837",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_titles_df['DOI'].tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf35788-a644-498a-8a0a-00ee8615aa68",
   "metadata": {},
   "source": [
    "## Investigating the Author Entries\n",
    "\n",
    "We'll start off by investigating the *author* field. This is an area that was found to have a number of potentially high priority issues as it pertains to social and political matters, as well as a field that has seen the some of the most pervasive issues in standardization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe43966",
   "metadata": {},
   "source": [
    "## Author Sequence\n",
    "Our first function will be checking the *sequence* sub-field within the *author* field. This is the field wherein authors are either listed as 'first' or 'addtional'. This function sets up a counter then iterates through the author list of a record to check what the noted sequence is for each author.\n",
    "\n",
    "The `try` block filters out records that have no authors listed. After that we begin to iterate through each author within a given record.\n",
    "\n",
    "`If 'name' in author.keys():` is used to filter out institutions listed as authors as using the 'name' key is often how an institution is presented as an author within the metadata record. The code within the `if` block simply says if there's an institution as an author and they are the only author listed, increase the counter to 1, then the code will continue down to the `return` statements where **0** will be returned as technically there is not an issue with sequence in that record.\n",
    "\n",
    "`else: if author['sequence'] == 'first'` block is where the bulk of the counting activity will happen. Up until this point we are mostly filtering out instances that don't apply to the problem at hand. Simply, the function will count how many authors are labled as 'first'. Once all authors of a record have been parsed, we go to the `return` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_checker(authorList):\n",
    "    counter = 0 \n",
    "    try: \n",
    "        for author in authorList:\n",
    "                if 'name' in author.keys():\n",
    "                    if len(authorList) == 1:\n",
    "                        counter +=1\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    if author['sequence'] == 'first':\n",
    "                        counter +=1\n",
    "                    else:\n",
    "                        continue\n",
    "        if counter == 0:\n",
    "            return 1 #no first author\n",
    "        elif len(authorList) > 1:\n",
    "            if counter > 1:\n",
    "                return 1 #multiple first authors\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0 #no issue\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c0c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'author' column need to be evaluated and formated before parsing,\n",
    "# otherwise they are treated as strings instead of dicts.\n",
    "import ast\n",
    "def reformat_col(record):\n",
    "    try:\n",
    "        formed = ast.literal_eval(record)\n",
    "        return formed\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "cols_to_reformat = ['author']\n",
    "for col in cols_to_reformat:\n",
    "    df[col] = df[col].apply(lambda x: reformat_col(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b80262",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author_sequence'] = df.author.map(lambda x: sequence_checker(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_with_AuSeq = df.loc[(df.author_sequence == 1)] #creating a df with only the cords with these errors\n",
    "prevalence_AuSeq = ((len(records_with_AuSeq))/(df.author.notnull().sum())) * 100\n",
    "#prevalence_AuSeq #returning a percent of the total number of records with this particular issue\n",
    "print(\"{:0.2f} percent of the article records contain more than one first author or no first author\".format(prevalence_AuSeq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f802002a",
   "metadata": {},
   "source": [
    "## Author Initials\n",
    "This function will utilize regular expressions for detecting the use of initials. Specifically, we are looking for when initials are used in totality, that is to say a record with \"Marianne E.\" will not be flagged, whereas a record with \"D.\" will.\n",
    "\n",
    "We look in both the 'given' and the 'family' sub-fields as this use of initials has been found in both sub-fields previously. \n",
    "\n",
    "The flow of the function operates similarly to the `sequence_checker`, we filter out records with `null` authors in the first `try` statement, followed by iteration through the author list, then another `try` statement where we filter out institutions as authors.\n",
    "\n",
    "The regular expressions can be broken into two conditions: `^(?:[A-Z]\\W{,3}\\s?){,3}` and `(?:[^\\W\\d_.]\\W){1,2}\\B` which are seperated by `|`. This is because each of those expressions are looking for initials, the former is looking in ASCII characters, whereas the latter s looking for the pattern in non-Latin characters.\n",
    "\n",
    "`if detector != None or len(author['given']) == 1` ensures that all initialized names are caught and then returned with the appropriate label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9459997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_initials_checker(authorList):\n",
    "    try: #Filter for no authors\n",
    "        for author in authorList: #iterating through author array\n",
    "            try: #filter for institutions as authors\n",
    "                detector = re.search(r\"^(?:[A-Z]\\W{,3}\\s?){1,3}$\", author['given']) #checking for initials in given\n",
    "                if detector or len(author['given']) == 1:\n",
    "                    return 1 #initials used\n",
    "                else:\n",
    "                    family_detector = re.search(r\"^(?:[A-Z]\\W{,3}\\s?){1,3}$\", author['family']) #initials in family\n",
    "                    if family_detector or len(author['family']) == 1:\n",
    "                        return 1 #initials used\n",
    "                    else:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "                        \n",
    "    except:\n",
    "        return None\n",
    "    return 0 #no issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942bb710",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author_initials'] = df.author.map(lambda x: author_initials_checker(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e7e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_with_initials = df.loc[df.author_initials == 1]\n",
    "prevalence_author_initials = ((len(records_with_initials))/(df.author.notnull().sum())) * 100\n",
    "#prevalence_author_initials # percentage of records with this specific issue\n",
    "print(\"{:0.2f} percent of the article records contain at least one author with only initials given\".format(prevalence_author_initials))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f28c121",
   "metadata": {},
   "source": [
    "## Institutions as Authors\n",
    "This function will address instances in which institutions are recorded as authors.\n",
    "\n",
    "`try:` will filter out records with `null` authors. Then we have the `institutions_present` list that looks for the telltale sign of an institution, the 'name' sub-field. \n",
    "\n",
    "If the list is populated with any authors, then the appropriate label signalling an institution will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6144469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def institution_as_author(authorList):\n",
    "    try:\n",
    "        institutions_present = [author for author in authorList if 'name' in author.keys()]\n",
    "        if len(institutions_present) > 0:\n",
    "            return 1 #institution as author\n",
    "        else:\n",
    "            return 0 #no issue\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a3da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author_institutions'] = df.author.map(lambda x: institution_as_author(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e780196",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_with_AuIns = df.loc[df.author_institutions == 1]\n",
    "prevalence_AuIns = ((len(records_with_AuIns))/(df.author.notnull().sum())) * 100\n",
    "#prevalence_AuIns #percentage of records with this specific issue\n",
    "print(\"{:0.2f} percent of the article records list at least one institution as an author\".format(prevalence_AuIns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c169e7",
   "metadata": {},
   "source": [
    "## Affiliation Missing\n",
    "\n",
    "This function will check if there is any data present within the Author `\"Affiliation\"` subfield.\n",
    "\n",
    "We start by creating a variable to operate as a indicator to the presence of an affiliation. We then iterate through each author within a given record.\n",
    "\n",
    "If an affiliation is present, we change the indicator to be `False`. After checking the authors, we assign `1` to records that are missing affiliations or `0` if there is no issue present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affiliations_missing(author_list):\n",
    "    no_affil = True\n",
    "    try:\n",
    "        for author in author_list:\n",
    "            affiliation = author['affiliation']\n",
    "            if len(affiliation) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                no_affil = False\n",
    "        if no_affil:\n",
    "            return 1\n",
    "        if not no_affil:\n",
    "            return 0\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368cffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['affiliation_missing'] = df.author.map(lambda x: affiliations_missing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78049ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_missing_affil = df.loc[df.affiliation_missing == 1]\n",
    "prevalence_miss_affil = ((len(records_missing_affil))/(df.author.notnull().sum())) * 100\n",
    "#prevalence_miss_affil\n",
    "print(\"{:0.2f} percent of the article records miss the affiliation for every author.\".format(prevalence_miss_affil))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4bf85e",
   "metadata": {},
   "source": [
    "## Checking for Honorifics in Author Names\n",
    "\n",
    "This function utilizes a set list of honorifics found, in Phase 1, to be used within the Author `\"Given\"` and `\"Family\"` subfields.\n",
    "\n",
    "After establishing our list, we then iterate through each Author of every record, putting their names into a lowercase format.\n",
    "\n",
    "We then check the given and family names for the use of the listed titles. Return `1` if an honorific is present. Return `0` if none are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acccf66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def honorific_checker(author_list):\n",
    "    titles_list = set(['dr.', 'prof', 'prof.', 'professor', 'doctor', 'dr', 'ingeniero'])\n",
    "    try:\n",
    "        for author in author_list:\n",
    "            lowercase_given = author['given'].lower()\n",
    "            lowercase_family = author['family'].lower()\n",
    "            if any(word in titles_list for word in lowercase_given.split()):\n",
    "                return 1\n",
    "            elif any(word in titles_list for word in lowercase_family.split()):\n",
    "                return 1\n",
    "            else: \n",
    "                continue\n",
    "        return 0\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f755a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author_honorific'] = df.author.map(lambda x: honorific_checker(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b4c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_with_honorific = df.loc[df.author_honorific == 1]\n",
    "prevalence_honorific = ((len(records_with_honorific))/df.author.notnull().sum()) * 100\n",
    "#prevalence_honorific\n",
    "print(\"At least {:0.2f} percent of the article records contain honorifics within the author names .\".format(prevalence_honorific))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d20b3e",
   "metadata": {},
   "source": [
    "## Uppercase Author Names\n",
    "\n",
    "With this function we will check each Author's `\"Given\"` and `\"Family\"` name subfields to see if the input is in all upercase letters.\n",
    "\n",
    "We start iterating through the author list and filter out records wherein the number of characters in a `\"Given\"` name is `1`. These are likely to be initials and as such are covered by another dimension of issue detection. We then use the regular expression `(?:^[A-Z]+)$` to return matches when an Author's name is in all uppercase letters. \n",
    "\n",
    "If a match is found we return `1` to signifiy the existence of an issue. If no match is returned, we repeat the process using the author's `\"Family\"` name. If no match is found for the `\"Family\"` name, then we proceed with the next author within the record until all authors have been checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad92850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uppercase_name(author_list):\n",
    "    try:\n",
    "        for author in author_list:\n",
    "            if len(author['given']) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                if re.match(r'(?:^[A-Z]+)$', author['given']):\n",
    "                    return 1\n",
    "                else:\n",
    "                    if len(author['family']) == 1:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if re.match(r'(?:^[A-Z]+)$', author['family']):\n",
    "                            return 1\n",
    "                        else:\n",
    "                            continue\n",
    "        return 0\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d324791",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author_uppercase'] = df.author.map(lambda x: uppercase_name(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cced143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_uppercase = df.loc[df.author_uppercase == 1]\n",
    "prevalence_uppercase = ((len(records_uppercase))/df.author.notnull().sum()) * 100\n",
    "#prevalence_uppercase\n",
    "print(\"{:0.2f} percent of the article records contain author names in all uppercase letters.\".format(prevalence_uppercase))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f518d78",
   "metadata": {},
   "source": [
    "## Non-Latin Characters\n",
    "\n",
    "This function detects the use of non-latin character sets. Particularly we are interested in practices of romanization and when it occurs: which journals, are the *language* fields present and accurate, and so on. \n",
    "\n",
    "First, we have to identify which records are using non-latin characters.\n",
    "\n",
    "This is split into two different functions. The first utilizes a regular expression `(?:[^ı́\\x00-\\xff])` to detect any characters not in ISO-8859-1 (or Latin-1) (See note).\n",
    "\n",
    "The second then utlizes the first function to then check each author within a given record.\n",
    "\n",
    "Note: This expression is providing a few too many false positives for my liking. I'm currently working on a better expression or a different solution entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcdca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isLatinChar(text):\n",
    "    regexp = re.compile(r'(?:[^ı́\\x00-\\xff])')\n",
    "    if regexp.search(text):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def latin_script_checker(authorList):\n",
    "    try:\n",
    "        latin_scripts = [author for author in authorList if isLatinChar(author['given'])]\n",
    "        if len(latin_scripts) > 0:\n",
    "            return 1 # non-latin script found\n",
    "        else:\n",
    "            return 0 # no issue\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f6ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author_characters'] = df.author.map(lambda x: latin_script_checker(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df746dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_with_non_latin = df.loc[df.author_characters == 1]\n",
    "prevalence_NonLatin = ((len(records_with_non_latin))/(df.author.notnull().sum())) * 100\n",
    "#prevalence_NonLatin #percentage of records with this specific issue\n",
    "print(\"{:0.2f} percent of the article records contain author (given) names in non-Latin letters.\".format(prevalence_NonLatin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03105fd4",
   "metadata": {},
   "source": [
    "## Abstract Multi-lingualism Detection\n",
    "This function will detect the use of more than one language within the *abstract* field. As mentioned before, we're interested in how people pracitice recording metadata as it pertains to language.\n",
    "\n",
    "Here we have a list of language ISO 639-1 codes. While it is not exhaustive (there are 183 offically assigned codes, and only 94 are present in this list), it does include many of the macrolanguages for which many other languages fall within.\n",
    "\n",
    "We pass this list to `langid` to ensure a higher confidence intervals in it's identification, i.e. an abstract might be in Malay (ms) but the identifier might return 'ms' and 'id' (Indonesian) with lower confidence intervals for each. As Malay is the macrolanguage that covers Indonesian, we will keep 'ms' but not 'id'.\n",
    "\n",
    "The first `try:` block filters for records without abstracts present, then we tokenize the abstracts by sentence.\n",
    "\n",
    "Next we pick out the first sentence and the second to last sentence of each abstract. The reason for picking out the second to last sentence is because most occurences of multi-lingual abstracts are such that the abstract is first written in one language, and then a second time in another. The reason for not picking the last sentence is because it is not uncommon for footnotes or citations to be present at the end of the abstracts in these metadata records. The presence of these at the end of an abstract section make language detection problematic as the syntactical structure can be odd and leads to an incorrect detection.\n",
    "\n",
    "We then classify both sentences, followed by an evaluation of the confidence intervals. If the confidence interval is especially low, it is omitted.\n",
    "\n",
    "We then check to see if there is more than one language present in the dictionary with `len(set(lang_dict.keys()))`, if so the record is returned with a **1**, indicating and error. Otherwise it is returned with a **0**.\n",
    "\n",
    "If this is the first time running this notebook, you may need to uncomment the top two lines of the cell:\n",
    "\n",
    "`import nltk`\n",
    "\n",
    "`nltk.download('punkt')`\n",
    "\n",
    "This is necessary for `sent_tokenize` to work as intended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "identifier = LanguageIdentifier.from_pickled_model(MODEL_FILE, norm_probs = True)\n",
    "lang_list = ['af', 'am', 'ar', 'as', 'az', 'be', 'bg', 'bn', 'br', \n",
    "             'bs', 'ca', 'cs', 'cy', 'da', 'de', 'dz', 'el', 'en', 'eo', \n",
    "             'es', 'et', 'eu', 'fa', 'fi', 'fo', 'fr', 'ga', 'gl', 'gu', \n",
    "             'he', 'hi', 'hr', 'ht', 'hu', 'hy', 'is', 'it', 'ja', 'jv', \n",
    "             'ka', 'kk', 'km', 'kn', 'ko', 'ku', 'ky', 'la', 'lb', 'lo', \n",
    "             'lt', 'lv', 'mg', 'mk', 'ml', 'mn', 'mr', 'ms', 'mt', 'ne', \n",
    "             'nl', 'no', 'oc', 'or', 'pa', 'pl', 'ps', 'pt', 'qu', 'ro', \n",
    "             'ru', 'rw', 'se', 'si', 'sk', 'sl', 'sq', 'sr', 'sv', 'sw', \n",
    "             'ta', 'te', 'th', 'tl', 'tr', 'ug', 'uk', 'ur', 'vi', 'vo', \n",
    "             'wa', 'xh', 'zh', 'zu']\n",
    "identifier.set_languages(langs=lang_list)\n",
    "def lang_checker(abstract):\n",
    "    try:\n",
    "        # Tokenizing abstracts\n",
    "        tokenized = sent_tokenize(abstract)\n",
    "        startAndFinish = [tokenized[0], tokenized[-1]]\n",
    "        # Detecting languages present\n",
    "        lang = [identifier.classify(lang) for lang in startAndFinish]\n",
    "        # Filter low confidence results\n",
    "        lang_dict = {key:value for (key,value) in lang if value > .95}\n",
    "        # Labeling specific issues found in record\n",
    "        if len(set(lang_dict.keys())) > 1:\n",
    "            return 1 #Multiple languages detected\n",
    "    except:\n",
    "        return None #No abstract\n",
    "    return 0 #No issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c771c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract_multi_lang']  = df.abstract.map(lambda x: lang_checker(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0dd8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_with_MultiLang = df.loc[(df.abstract_multi_lang == 1)]\n",
    "prevalence_MultiLang = ((len(records_with_MultiLang))/(df.abstract.notnull().sum())) * 100\n",
    "#prevalence_MultiLang #returning a percent of the total number of records with this particular issue\n",
    "print(\"{:0.2f} percent of the article records contain abstracts that contain more than one language.\".format(prevalence_MultiLang))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e54b1e",
   "metadata": {},
   "source": [
    "## Title Language Checking\n",
    "This function will check the language of the title against the stated language of the record.\n",
    "\n",
    "It is a relatively striaghtforward function: `try:` filters out records without a *title*, then classifies the language, and finally checks to see if the returned code matches what is record in the language field.\n",
    "\n",
    "We use `df.apply` instead of `df.column.map` because of the need to check multiple fields within a record as opposed to being contained within a specific field.\n",
    "\n",
    "Here, it should be mentioned, there is some abiguity. The *language* field is not clearly defined (is it the language of the Item, Container, or the record). The prevelance of this issue (seen below) reflects the lack of clarity in what this field is meant to represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf0d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_lang_checker(record):\n",
    "    try:\n",
    "        if str(record['title']).lower() != 'nan':\n",
    "            lang = identifier.classify(record['title'])\n",
    "            if lang[1] > .99:\n",
    "                if lang[0] == record['language']:\n",
    "                    return 0\n",
    "                else:\n",
    "                    if str(record['language']).lower() == 'nan':\n",
    "                        return None\n",
    "                    else:\n",
    "                        return 1\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da95282",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_language'] = df.apply(title_lang_checker, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04787a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_with_TitleLang = df.loc[(df.title_language == 1)] #creating a df with only the records with these errors\n",
    "prevalence_TitleLang = ((len(records_with_TitleLang))/len(df.loc[(df.title.notnull()) & (df.language.notnull())])) * 100\n",
    "#prevalence_TitleLang #returning a percent of the total number of records with this particular issue\n",
    "print(\"{:0.2f} percent of the article records have a mismatch between the given language and the detected language of the article title.\".format(prevalence_TitleLang))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea01fe6e",
   "metadata": {},
   "source": [
    "## Total Errors\n",
    "Lastly, we'll add up all of the errors for each record and store them number in *'total_errors'* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174bef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labled Columns\n",
    "column_list = ['no_author', 'no_language', 'no_title', 'author_sequence', 'author_initials', 'author_institutions',\n",
    "              'author_characters', 'abstract_multi_lang', 'title_language', 'author_uppercase',\n",
    "              'affiliation_missing', 'author_honorific']\n",
    "df['total_errors'] = df[column_list].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a77453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking a look at the df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf560bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(output_dir / '03_labeled_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
